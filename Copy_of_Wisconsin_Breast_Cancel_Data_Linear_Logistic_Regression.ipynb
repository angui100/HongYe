{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Wisconsin_Breast_Cancel_Data_Linear_Logistic_Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMH6HCLYS+muJ81af6JPubp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angui100/HongYe/blob/master/Copy_of_Wisconsin_Breast_Cancel_Data_Linear_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMm5E1Ex4384",
        "colab_type": "text"
      },
      "source": [
        "## Linear Classification / Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAKdFVEJ48NG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing lib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "#from sklearn.cross_validation import \n",
        "from sklearn.model_selection import train_test_split, cross_val_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCUyNsXS98Uj",
        "colab_type": "code",
        "outputId": "d9a81e3d-d508-4a86-caff-13e149390011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#prepare for data file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwN016x0-ArB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_file_path = '/content/drive/My Drive/dataset/breastCancer.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGekB3m85BsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LinearClassification(object):\n",
        "    \n",
        "    def __init__(self):\n",
        "        #defining hyperparams\n",
        "        self.learning_rate = 0.0001\n",
        "        self.batch_size = 200\n",
        "        self.no_of_iter = 1000\n",
        "        #videcu za ovaj\n",
        "        self.reg = 0.000001\n",
        "        \n",
        "    \n",
        "    #Input NOTE: X - matrix of data, can be used on images or numerical data (N x D)\n",
        "    #          N - Number of samples, D - Number of features\n",
        "    #          In case you use images make sure that X.shape[0] represent NUMBER of samples\n",
        "    #          y - labels (Nx1)\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "        \n",
        "        #0 notation - so we add + 1 to max value from y\n",
        "        self.no_of_classes = np.max(y) + 1\n",
        "        \n",
        "        #defining hyperparams\n",
        "        # W - matrix of weights (No_of_classes x No_of_features)\n",
        "        self.W = np.random.rand(self.no_of_classes, self.X_train.shape[1]) * 0.001\n",
        "        \n",
        "        self.W, loss_history = self.SGD(self.W, self.X_train, self.y_train, self.learning_rate, self.batch_size, self.no_of_iter, self.reg)\n",
        "        \n",
        "        return loss_history\n",
        "    \n",
        "    #STOCHASTIC GRADIENT DESCENT\n",
        "    #Inputs: W - weights that we are trying to update\n",
        "    #        X - feautere of training set\n",
        "    #        y - wanted labels\n",
        "    #        learning_rate - how fast it is going to find good parameters\n",
        "    #        batch_size - how big PART of training set algo is using per iter\n",
        "    #        no_of_iter -  how many times it is going to run\n",
        "    #        reg - regularization\n",
        "    #\n",
        "    #Outputs: W_updated - updated weights matrix acording to loss function used\n",
        "    #         loss_history for verbose reptresentation of our loss computation\n",
        "    def SGD(self, W, X, y, learning_rate, batch_size, no_of_iter, reg):\n",
        "        W_updated = W\n",
        "        \n",
        "        no_of_train = X.shape[0]\n",
        "        #It is not necessities, but we can define loss_hitory to be sure that algo is working good\n",
        "        loss_history = []\n",
        "        \n",
        "        for i in range(no_of_iter):\n",
        "            batch_inx = np.random.choice(no_of_train, batch_size, replace=True)\n",
        "            #creting smallers train sets to fit in our SGD\n",
        "            X_batch = X[batch_inx,:]\n",
        "            y_batch = y[batch_inx]\n",
        "            \n",
        "            \n",
        "            loss, grad = self.SVM_classfier(W_updated, X_batch, y_batch, reg)\n",
        "            loss_history.append(loss)\n",
        "            #Update W:\n",
        "            W_updated = W_updated - (learning_rate * grad)\n",
        "            \n",
        "        return W_updated, loss_history\n",
        "            \n",
        "    #Inputs: W - current weights\n",
        "    #        X - training set features\n",
        "    #        y - training set labels\n",
        "    #        reg - regularization strenght\n",
        "    #\n",
        "    #Outputs: gradient_W - values to updated starting W\n",
        "    #         loss - to see if we are updaing in good direction\n",
        "    def SVM_classfier(self, W, X, y, reg):\n",
        "        \n",
        "        no_of_classes = np.max(y) + 1\n",
        "        #creating matrix with zeros, same shape as starting weights\n",
        "        \n",
        "        gradient_W = np.zeros(W.shape)\n",
        "        \n",
        "        loss = 0.0 \n",
        "        for i in range(X.shape[0]):\n",
        "            #First we need to multiply weights and x for particular sample\n",
        "            #need to transpose to long vector current sample\n",
        "            scores = W.dot(X[i, :].T)\n",
        "            #we are getting values for currect class\n",
        "            correct_class = scores[y[i]]\n",
        "            for j in range(no_of_classes):\n",
        "                if j == y[i]:\n",
        "                    continue\n",
        "                # This is simple formula for SVM\n",
        "                current_class_margin = scores[j] - correct_class + 1 #one is \n",
        "                if current_class_margin > 0:\n",
        "                    loss +=  current_class_margin\n",
        "                \n",
        "                    gradient_W[y[i]:1, :] -= X[i, :] #This is where we are creating gradient for CURRECT class\n",
        "                    gradient_W[j:1, :] += X[y[i], :]\n",
        "        \n",
        "        #average over number of train samples\n",
        "        loss /= X.shape[0]\n",
        "        gradient_W /= X.shape[0]\n",
        "        \n",
        "        loss += 0.5 * reg * np.sum(W * W)\n",
        "        \n",
        "        gradient_W += reg*W\n",
        "    \n",
        "        return loss, gradient_W\n",
        "    \n",
        "    #Predict function\n",
        "    #Input: X - test set \n",
        "    #\n",
        "    #Output: predict - list of classes\n",
        "    def predict(self, X):\n",
        "        pred = []\n",
        "        for i in range(X.shape[0]):\n",
        "            pred.append(np.argmax(np.dot(self.W,X[i, :].T)))\n",
        "        return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rescH1oM7W77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to check how much did algo predict right\n",
        "def accuracy(y_tes, y_pred):\n",
        "    correct = 0\n",
        "    for i in range(len(y_pred)):\n",
        "        if(y_tes[i] == y_pred[i]):\n",
        "            correct += 1\n",
        "    return (correct/len(y_tes))*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKu8_vLT7e9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run():\n",
        "    # Importing the dataset\n",
        "    dataset = pd.read_csv(data_file_path)\n",
        "    dataset.replace('?', 0, inplace=True)\n",
        "    dataset = dataset.applymap(np.int64)\n",
        "    X = dataset.iloc[:, 1:-1].values    \n",
        "    y = dataset.iloc[:, -1].values\n",
        "    #handling labels column\n",
        "    y_new = []\n",
        "    for i in range(len(y)):\n",
        "        if y[i] == 2:\n",
        "            y_new.append(0)\n",
        "        else:\n",
        "            y_new.append(1)\n",
        "    y_new = np.array(y_new)\n",
        "\n",
        "    \n",
        "    # Splitting the dataset into the Training set and Test set\n",
        "    #from sklearn.cross_validation import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y_new, test_size = 0.25, random_state = 0)\n",
        "    \n",
        "\n",
        "    # Feature Scaling\n",
        "#     from sklearn.preprocessing import StandardScaler\n",
        "#     sc = StandardScaler()\n",
        "#     X_train = sc.fit_transform(X_train)\n",
        "#     X_test = sc.transform(X_test)\n",
        "\n",
        "   \n",
        "    classifier = LinearClassification()\n",
        "    loss_history = classifier.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred = classifier.predict(X_test)\n",
        "    \n",
        "    #Sklearn test\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    reg = LogisticRegression(random_state=0)\n",
        "    reg.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred_sk = reg.predict(X_test)\n",
        "\n",
        "# Uncomment if you want to print out losses\n",
        "#     for i in range(len(loss_history)):\n",
        "#         print(loss_history[i])\n",
        "    \n",
        "    print(\"My algorithm on this dataset: \",accuracy(y_test, y_pred), \"%\")\n",
        "    print(\"Sklearn Logistic regression score: \",accuracy(y_test, y_pred_sk),\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAfHpM9d78t1",
        "colab_type": "code",
        "outputId": "5d88ecb8-d948-4718-de54-1d4d3c2044c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "My algorithm on this dataset:  77.14285714285715 %\n",
            "Sklearn Logistic regression score:  96.57142857142857 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}